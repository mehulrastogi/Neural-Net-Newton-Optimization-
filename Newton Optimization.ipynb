{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has an example of Neton Optimization tecnique on a Multilayer Perceptron using NumPy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but first wee need to undestand what is a Newton Optimization Technique and how is it dfferent from the Gradient Descent Tecnique gernally used for optimizing a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am building a normal forward propogation for a single perceptron\n",
    "# and the various function i would use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    #this is the non-linear activation function\n",
    "    return (1 / (1 + np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Declaring Traing Data        ############\n",
    "#############################################\n",
    "X_train = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "Y_train = np.array([[1],[0],[0],[1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    '''\n",
    "    Class containing loss functions\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def sq_loss(self , pred ,Y):\n",
    "        loss = Y - pred\n",
    "        d_loss = -1 * 2 * loss\n",
    "        loss = np.sum(loss * loss)\n",
    "        \n",
    "        return loss,d_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Layer:\n",
    "    '''\n",
    "    This is a very specefic Neuron class that uses sigmoid activation\n",
    "    and square loss.\n",
    "    '''\n",
    "    def __init__(self,l0,l1):\n",
    "        self.W = np.random.randn(l0 ,l1) * np.sqrt(2.0 /l0 )\n",
    "        self.b = np.zeros((l1))\n",
    "        print(self.W.shape , self.b.shape)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        #print(X.shape , self.W.shape , self.b.shape)\n",
    "        h_x = X.dot(self.W) + self.b\n",
    "        h_x = sigmoid(h_x)\n",
    "        return h_x\n",
    "    \n",
    "    def update_newton(self,X,h_x,d_back):\n",
    "        \n",
    "        #f_x = self.forward(X) \n",
    "        f_x = h_x\n",
    "        \n",
    "        #Dirivating w.r.t W\n",
    "        temp = f_x * (1 - f_x) * d_back #this value was computed again and again\n",
    "        f_x_dash_W = X.T.dot(temp)\n",
    "        f_x_dash_dash_W = (X.T.dot(1 - (2 * f_x))) * f_x_dash_W\n",
    "        \n",
    "        #Derivatng w.r.t b\n",
    "        f_x_dash_b = temp\n",
    "        f_x_dash_dash_b = f_x * (1 - 2 * f_x_dash_b)\n",
    "        \n",
    "        #Derivating w.r.t x(to return to previous layers)\n",
    "        f_x_dash_x = temp.dot(self.W.T)   \n",
    "        \n",
    "        delta_W = -1 * (f_x_dash_W / f_x_dash_dash_W)\n",
    "        self.W = self.W + delta_W\n",
    "        \n",
    "        delta_b = -1 * np.sum(f_x_dash_b / f_x_dash_dash_b , axis = 0)\n",
    "        self.b = self.b + delta_b\n",
    "        \n",
    "        d_back = f_x_dash_x\n",
    "        \n",
    "        return d_back\n",
    "        \n",
    "    def update_batch_gradient_descent(self , X , h_x , d_back,alpha = 0.01):\n",
    "        \n",
    "        f_x = h_x\n",
    "        \n",
    "        temp = f_x * (1 - f_x) * d_back\n",
    "        #Dirivating w.r.t W\n",
    "        f_x_dash_W = X.T.dot(temp)\n",
    "        \n",
    "        #Derivatng w.r.t b\n",
    "        f_x_dash_b = temp\n",
    "        \n",
    "        #Derivating w.r.t x(to return to previous layers)\n",
    "        f_x_dash_x = temp.dot(self.W.T)\n",
    "        \n",
    "        #Update W\n",
    "        delta_W = f_x_dash_W\n",
    "        self.W = self.W - alpha * delta_W\n",
    "        \n",
    "        #Update b\n",
    "        delta_b = np.sum(f_x_dash_b , axis =0)\n",
    "        self.b = self.b - alpha * delta_b\n",
    "        \n",
    "        d_back = f_x_dash_x\n",
    "        return d_back\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model , X , Y):\n",
    "    pred = model.forward(X)\n",
    "    pred =pred > 0.5\n",
    "    acc = np.sum(pred == Y)\n",
    "    acc = acc / Y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1) (1,)\n"
     ]
    }
   ],
   "source": [
    "# Declare a neuron with shape of weights as [shape_of_input,1]\n",
    "model = Neural_Layer(2,1)\n",
    "loss_m = Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       ],\n",
       "       [0.67857825],\n",
       "       [0.76411289],\n",
       "       [0.87242896]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the model outputs\n",
    "model.forward(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 0th epoch : 1.3106113195202735  Training Accuracy:0.25\n",
      "The training loss at 1th epoch : 1.3438342091441138  Training Accuracy:0.25\n",
      "The training loss at 2th epoch : 1.443314375929998  Training Accuracy:0.25\n",
      "The training loss at 3th epoch : 1.7301217110075013  Training Accuracy:0.25\n",
      "The training loss at 4th epoch : 1.8885918079033581  Training Accuracy:0.25\n",
      "The training loss at 5th epoch : 2.1346740465999874  Training Accuracy:0.25\n",
      "The training loss at 6th epoch : 2.5371416633133386  Training Accuracy:0.25\n",
      "The training loss at 7th epoch : 2.812321370831639  Training Accuracy:0.25\n",
      "The training loss at 8th epoch : 2.6509277119898798  Training Accuracy:0.25\n",
      "The training loss at 9th epoch : 2.414860258262007  Training Accuracy:0.25\n"
     ]
    }
   ],
   "source": [
    "# Now we can train the model by training on a batch.As our batch is equal to the raing data.1 iter = 1 epoch\n",
    "\n",
    "\n",
    "def train_newton_update(model ,loss_m,X_train , Y_train,epochs):\n",
    "    for i in range(epochs):\n",
    "        h_x = model.forward(X_train)\n",
    "        loss,d_loss = loss_m.sq_loss(h_x,Y_train)\n",
    "        acc = accuracy(model ,X_train,Y_train)\n",
    "        print(\"The training loss at {}th epoch : {}  Training Accuracy:{}\".format(i , loss , acc))\n",
    "        \n",
    "        model.update_newton(X_train,h_x , d_loss)        \n",
    "\n",
    "train_newton_update(model , loss_m , X_train,Y_train , 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can predict the values for unseen data or trained data also\n",
    "# We can also calculate the accuracy of the model we have trained\n",
    "accuracy(model , X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1) (1,)\n",
      "The training loss at 0th epoch : 1.1101920569652854  Training Accuracy:0.25\n",
      "The training loss at 1th epoch : 1.479373641896645  Training Accuracy:0.25\n",
      "The training loss at 2th epoch : 1.4425253176135537  Training Accuracy:0.25\n",
      "The training loss at 3th epoch : 1.7456017414542437  Training Accuracy:0.25\n",
      "The training loss at 4th epoch : 1.9063977163803227  Training Accuracy:0.25\n",
      "The training loss at 5th epoch : 2.1628098036600067  Training Accuracy:0.25\n",
      "The training loss at 6th epoch : 2.57444484423525  Training Accuracy:0.25\n",
      "The training loss at 7th epoch : 2.8112514361582925  Training Accuracy:0.25\n",
      "The training loss at 8th epoch : 2.625739273936632  Training Accuracy:0.25\n",
      "The training loss at 9th epoch : 2.397586951638596  Training Accuracy:0.25\n"
     ]
    }
   ],
   "source": [
    "model = Neural_Layer(2,1)\n",
    "loss_m = Loss()\n",
    "\n",
    "def train_gradient_update(model ,loss_m,X_train , Y_train,epochs):\n",
    "    for i in range(epochs):\n",
    "        h_x = model.forward(X_train)\n",
    "        loss,d_loss = loss_m.sq_loss(h_x,Y_train)\n",
    "        acc = accuracy(model ,X_train,Y_train)\n",
    "        print(\"The training loss at {}th epoch : {}  Training Accuracy:{}\".format(i , loss , acc))\n",
    "        \n",
    "        model.update_batch_gradient_descent(X_train,h_x , d_loss , 0.01)        \n",
    "\n",
    "train_newton_update(model , loss_m , X_train,Y_train , 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can predict the values for unseen data or trained data also\n",
    "# We can also calculate the accuracy of the model we have trained\n",
    "accuracy(model , X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built a single perceptron layer we can scale it to make a neural net(A deep Neral Net would just be adding more layers on it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to make a Neural Network that is scalable from scratch.That is it can be directly used as a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Net:\n",
    "    \n",
    "    def __init__(self , layer_list):\n",
    "        \n",
    "        \n",
    "        self.hidden_layers = len(layer_list) - 1\n",
    "        \n",
    "        self.layers = {}\n",
    "        \n",
    "        for i in range(self.hidden_layers):            \n",
    "            self.layers[i+1] = Neural_Layer(layer_list[i] , layer_list[i+1])\n",
    "            \n",
    "            \n",
    "    def forward(self , X_train):\n",
    "        t = X_train\n",
    "        cache = []\n",
    "        cache.append(t)        \n",
    "        for i in range(self.hidden_layers):\n",
    "            t = self.layers[i+1].forward(t)\n",
    "            cache.append(t)\n",
    "            \n",
    "        return t,cache\n",
    "    \n",
    "    def update_newton(self , cache, d_back):\n",
    "        #list of activations--cache\n",
    "        \n",
    "        for i in range(self.hidden_layers , 0 , -1):\n",
    "            d_back = self.layers[i].update_newton(cache[i-1],cache[i],d_back)\n",
    "            \n",
    "        return d_back \n",
    "    \n",
    "    def update_gradient(self , cache, d_back , alpha=0.01):\n",
    "        for i in range(self.hidden_layers , 0 , -1):\n",
    "            d_back = self.layers[i].update_batch_gradient_descent(cache[i-1],cache[i],d_back,alpha)\n",
    "            \n",
    "        return d_back \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_neural_net(model , X , Y):\n",
    "    pred,_ = model.forward(X)\n",
    "    pred =pred > 0.5\n",
    "    acc = np.sum(pred == Y)\n",
    "    acc = acc / Y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4) (4,)\n",
      "(4, 2) (2,)\n",
      "(2, 1) (1,)\n"
     ]
    }
   ],
   "source": [
    "# Lets make a neural net as follows\n",
    "# visible_layer-->3 neurons-->1 neuron\n",
    "layer_list = [2 , 4,2 , 1]\n",
    "model = Neural_Net(layer_list)\n",
    "loss_m = Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59624879],\n",
       "       [0.57570382],\n",
       "       [0.62992417],\n",
       "       [0.60762873]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred,_ = model.forward(X_train)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 0th epoch : 1.0452096025974364  Training Accuracy:0.5\n",
      "The training loss at 1th epoch : 1.185441402352394  Training Accuracy:0.5\n"
     ]
    }
   ],
   "source": [
    "def train_newton_update(model ,loss_m,X_train , Y_train,epochs):\n",
    "    for i in range(epochs):\n",
    "        act , cache = model.forward(X_train)\n",
    "        loss,d_back = loss_m.sq_loss(act,Y_train)\n",
    "        acc = accuracy_neural_net(model ,X_train,Y_train)\n",
    "        print(\"The training loss at {}th epoch : {}  Training Accuracy:{}\".format(i , loss , acc))\n",
    "        model.update_newton(cache,d_back)        \n",
    "\n",
    "train_newton_update(model , loss_m , X_train,Y_train , 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_neural_net(model,X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2) (2,)\n",
      "(2, 2) (2,)\n",
      "(2, 1) (1,)\n",
      "The training loss at 0th epoch : 1.0338127587302044  Training Accuracy:0.5\n",
      "The training loss at 1th epoch : 1.033147473506944  Training Accuracy:0.5\n"
     ]
    }
   ],
   "source": [
    "layer_list = [2 ,2,2, 1]\n",
    "model = Neural_Net(layer_list)\n",
    "loss_m = Loss()\n",
    "\n",
    "def train_gradient_update(model ,loss_m,X_train , Y_train,epochs,alpha =0.01):\n",
    "    for i in range(epochs):\n",
    "        act , cache = model.forward(X_train)\n",
    "        loss,d_back = loss_m.sq_loss(act,Y_train)\n",
    "        acc = accuracy_neural_net(model ,X_train,Y_train)\n",
    "        print(\"The training loss at {}th epoch : {}  Training Accuracy:{}\".format(i , loss , acc))\n",
    "        model.update_gradient(cache,d_back,alpha)        \n",
    "\n",
    "train_gradient_update(model , loss_m , X_train,Y_train , 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58515536],\n",
       "       [0.60871105],\n",
       "       [0.5778231 ],\n",
       "       [0.60504627]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred,_ = model.forward(X_train)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_neural_net(model,X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N Bit XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Lets try working with just a little better data. A n XOR operator. So lets create the dataset for n bit xor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would have atmost 2^n data point in this type of data set.BUt we would limit our dataset to a 1000 data points\n",
    "whichever is smaller.\n",
    "\n",
    "Then we can divide into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 10\n",
    "max_datapoint = 1000\n",
    "datapoints = min(pow(2,n) , max_datapoint)\n",
    "\n",
    "X = np.zeros((datapoints , n) , dtype=np.int32)\n",
    "Y = np.zeros((datapoints , 1), dtype=np.int32)\n",
    "\n",
    "for i in range(datapoints):\n",
    "    tmp = i\n",
    "    y_tmp = 0\n",
    "    for j in range(n-1 , -1 , -1):\n",
    "        X[i,j] = tmp&1\n",
    "        y_tmp = y_tmp^X[i,j]\n",
    "        tmp = tmp>>1\n",
    "    Y[i] = y_tmp\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 1] [0]\n"
     ]
    }
   ],
   "source": [
    "# for sanity check lets print one example\n",
    "ind = 3\n",
    "print(X[ind] , Y[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets divide the set in training and testing\n",
    "div = 0.9\n",
    "train_n = int(div * datapoints)\n",
    "X_train = X[:train_n]\n",
    "Y_train = Y[:train_n]\n",
    "\n",
    "X_test = X[train_n:]\n",
    "Y_test = Y[train_n:]\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 8) (8,)\n",
      "(8, 4) (4,)\n",
      "(4, 2) (2,)\n",
      "(2, 1) (1,)\n"
     ]
    }
   ],
   "source": [
    "layer_list = [n ,8,4,2, 1]\n",
    "model = Neural_Net(layer_list)\n",
    "loss_m = Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 0th epoch : 231.43765663008276  Training Accuracy:0.5\n",
      "The training loss at 1th epoch : 230.15033331382855  Training Accuracy:0.5\n",
      "The training loss at 2th epoch : 228.5914891322086  Training Accuracy:0.5\n",
      "The training loss at 3th epoch : 227.75048674115249  Training Accuracy:0.5\n",
      "The training loss at 4th epoch : 226.9178514994976  Training Accuracy:0.5\n",
      "The training loss at 5th epoch : 226.43296801232856  Training Accuracy:0.5\n",
      "The training loss at 6th epoch : 226.00542986289054  Training Accuracy:0.5\n",
      "The training loss at 7th epoch : 225.74247721813376  Training Accuracy:0.5\n",
      "The training loss at 8th epoch : 225.52675940861195  Training Accuracy:0.5\n",
      "The training loss at 9th epoch : 225.38875019627397  Training Accuracy:0.5\n",
      "The training loss at 10th epoch : 225.2807456633013  Training Accuracy:0.5\n",
      "The training loss at 11th epoch : 225.209654812245  Training Accuracy:0.5\n",
      "The training loss at 12th epoch : 225.15575800113382  Training Accuracy:0.5011111111111111\n",
      "The training loss at 13th epoch : 225.11954794811555  Training Accuracy:0.49777777777777776\n",
      "The training loss at 14th epoch : 225.09268379972045  Training Accuracy:0.5\n",
      "The training loss at 15th epoch : 225.07436472906235  Training Accuracy:0.49777777777777776\n",
      "The training loss at 16th epoch : 225.06097098170147  Training Accuracy:0.5\n",
      "The training loss at 17th epoch : 225.0517338305286  Training Accuracy:0.5022222222222222\n",
      "The training loss at 18th epoch : 225.04504165997588  Training Accuracy:0.5\n",
      "The training loss at 19th epoch : 225.04038151331122  Training Accuracy:0.5\n",
      "The training loss at 20th epoch : 225.0370189116322  Training Accuracy:0.49777777777777776\n",
      "The training loss at 21th epoch : 225.0346530095219  Training Accuracy:0.5022222222222222\n",
      "The training loss at 22th epoch : 225.0329423919594  Training Accuracy:0.5011111111111111\n",
      "The training loss at 23th epoch : 225.0317216289952  Training Accuracy:0.4988888888888889\n",
      "The training loss at 24th epoch : 225.03082950653345  Training Accuracy:0.5033333333333333\n",
      "The training loss at 25th epoch : 225.03017840322704  Training Accuracy:0.49777777777777776\n",
      "The training loss at 26th epoch : 225.02969129540347  Training Accuracy:0.5011111111111111\n",
      "The training loss at 27th epoch : 225.02932286504813  Training Accuracy:0.5044444444444445\n",
      "The training loss at 28th epoch : 225.02903600640013  Training Accuracy:0.49777777777777776\n",
      "The training loss at 29th epoch : 225.0288076536428  Training Accuracy:0.5\n",
      "The training loss at 30th epoch : 225.02861983361896  Training Accuracy:0.4988888888888889\n",
      "The training loss at 31th epoch : 225.02846093804703  Training Accuracy:0.5\n",
      "The training loss at 32th epoch : 225.0283222327847  Training Accuracy:0.5022222222222222\n",
      "The training loss at 33th epoch : 225.0281979233868  Training Accuracy:0.49777777777777776\n",
      "The training loss at 34th epoch : 225.02808375849673  Training Accuracy:0.5044444444444445\n",
      "The training loss at 35th epoch : 225.027976863101  Training Accuracy:0.49777777777777776\n",
      "The training loss at 36th epoch : 225.02787516672066  Training Accuracy:0.5022222222222222\n",
      "The training loss at 37th epoch : 225.02777724902577  Training Accuracy:0.4988888888888889\n",
      "The training loss at 38th epoch : 225.02768210169472  Training Accuracy:0.5011111111111111\n",
      "The training loss at 39th epoch : 225.02758902494313  Training Accuracy:0.5022222222222222\n",
      "The training loss at 40th epoch : 225.02749752770876  Training Accuracy:0.4988888888888889\n",
      "The training loss at 41th epoch : 225.0274072654562  Training Accuracy:0.5011111111111111\n",
      "The training loss at 42th epoch : 225.02731799877924  Training Accuracy:0.5\n",
      "The training loss at 43th epoch : 225.027229557645  Training Accuracy:0.5\n",
      "The training loss at 44th epoch : 225.02714182488285  Training Accuracy:0.5022222222222222\n",
      "The training loss at 45th epoch : 225.02705471593174  Training Accuracy:0.5\n",
      "The training loss at 46th epoch : 225.0269681728674  Training Accuracy:0.5022222222222222\n",
      "The training loss at 47th epoch : 225.02688215290664  Training Accuracy:0.5022222222222222\n",
      "The training loss at 48th epoch : 225.02679662677005  Training Accuracy:0.5022222222222222\n",
      "The training loss at 49th epoch : 225.02671157207305  Training Accuracy:0.5022222222222222\n",
      "The training loss at 50th epoch : 225.02662697334904  Training Accuracy:0.5022222222222222\n",
      "The training loss at 51th epoch : 225.02654281817644  Training Accuracy:0.5022222222222222\n",
      "The training loss at 52th epoch : 225.02645909773332  Training Accuracy:0.5022222222222222\n",
      "The training loss at 53th epoch : 225.02637580447654  Training Accuracy:0.5022222222222222\n",
      "The training loss at 54th epoch : 225.02629293277624  Training Accuracy:0.5022222222222222\n",
      "The training loss at 55th epoch : 225.02621047749324  Training Accuracy:0.5022222222222222\n",
      "The training loss at 56th epoch : 225.02612843453358  Training Accuracy:0.5022222222222222\n",
      "The training loss at 57th epoch : 225.026046799958  Training Accuracy:0.5022222222222222\n",
      "The training loss at 58th epoch : 225.0259655704179  Training Accuracy:0.5022222222222222\n",
      "The training loss at 59th epoch : 225.02588474258732  Training Accuracy:0.5022222222222222\n",
      "The training loss at 60th epoch : 225.02580431348784  Training Accuracy:0.5022222222222222\n",
      "The training loss at 61th epoch : 225.02572428012053  Training Accuracy:0.5022222222222222\n",
      "The training loss at 62th epoch : 225.0256446397002  Training Accuracy:0.5022222222222222\n",
      "The training loss at 63th epoch : 225.0255653894143  Training Accuracy:0.5022222222222222\n",
      "The training loss at 64th epoch : 225.02548652658828  Training Accuracy:0.5022222222222222\n",
      "The training loss at 65th epoch : 225.02540804852617  Training Accuracy:0.5022222222222222\n",
      "The training loss at 66th epoch : 225.0253299526258  Training Accuracy:0.5022222222222222\n",
      "The training loss at 67th epoch : 225.02525223627276  Training Accuracy:0.5022222222222222\n",
      "The training loss at 68th epoch : 225.02517489691974  Training Accuracy:0.5022222222222222\n",
      "The training loss at 69th epoch : 225.0250979320155  Training Accuracy:0.5022222222222222\n",
      "The training loss at 70th epoch : 225.02502133905978  Training Accuracy:0.5022222222222222\n",
      "The training loss at 71th epoch : 225.024945115555  Training Accuracy:0.5022222222222222\n",
      "The training loss at 72th epoch : 225.02486925904375  Training Accuracy:0.5022222222222222\n",
      "The training loss at 73th epoch : 225.02479376707672  Training Accuracy:0.5022222222222222\n",
      "The training loss at 74th epoch : 225.02471863723775  Training Accuracy:0.5022222222222222\n",
      "The training loss at 75th epoch : 225.02464386712234  Training Accuracy:0.5022222222222222\n",
      "The training loss at 76th epoch : 225.02456945435443  Training Accuracy:0.5022222222222222\n",
      "The training loss at 77th epoch : 225.024495396572  Training Accuracy:0.5033333333333333\n",
      "The training loss at 78th epoch : 225.02442169143853  Training Accuracy:0.5033333333333333\n",
      "The training loss at 79th epoch : 225.02434833663284  Training Accuracy:0.5033333333333333\n",
      "The training loss at 80th epoch : 225.02427532985712  Training Accuracy:0.5033333333333333\n",
      "The training loss at 81th epoch : 225.0242026688299  Training Accuracy:0.5033333333333333\n",
      "The training loss at 82th epoch : 225.02413035129135  Training Accuracy:0.5033333333333333\n",
      "The training loss at 83th epoch : 225.02405837499867  Training Accuracy:0.5033333333333333\n",
      "The training loss at 84th epoch : 225.02398673772936  Training Accuracy:0.5033333333333333\n",
      "The training loss at 85th epoch : 225.02391543727825  Training Accuracy:0.5033333333333333\n",
      "The training loss at 86th epoch : 225.0238444714596  Training Accuracy:0.5033333333333333\n",
      "The training loss at 87th epoch : 225.023773838105  Training Accuracy:0.5033333333333333\n",
      "The training loss at 88th epoch : 225.0237035350647  Training Accuracy:0.5033333333333333\n",
      "The training loss at 89th epoch : 225.02363356020624  Training Accuracy:0.5033333333333333\n",
      "The training loss at 90th epoch : 225.0235639114153  Training Accuracy:0.5033333333333333\n",
      "The training loss at 91th epoch : 225.02349458659455  Training Accuracy:0.5033333333333333\n",
      "The training loss at 92th epoch : 225.02342558366442  Training Accuracy:0.5033333333333333\n",
      "The training loss at 93th epoch : 225.02335690056196  Training Accuracy:0.5033333333333333\n",
      "The training loss at 94th epoch : 225.02328853524156  Training Accuracy:0.5033333333333333\n",
      "The training loss at 95th epoch : 225.02322048567413  Training Accuracy:0.5033333333333333\n",
      "The training loss at 96th epoch : 225.02315274984733  Training Accuracy:0.5033333333333333\n",
      "The training loss at 97th epoch : 225.02308532576515  Training Accuracy:0.5033333333333333\n",
      "The training loss at 98th epoch : 225.02301821144795  Training Accuracy:0.5033333333333333\n",
      "The training loss at 99th epoch : 225.0229514049321  Training Accuracy:0.5033333333333333\n",
      "The training loss at 100th epoch : 225.02288490426997  Training Accuracy:0.5033333333333333\n",
      "The training loss at 101th epoch : 225.02281870752978  Training Accuracy:0.5033333333333333\n",
      "The training loss at 102th epoch : 225.02275281279526  Training Accuracy:0.5033333333333333\n",
      "The training loss at 103th epoch : 225.0226872181657  Training Accuracy:0.5022222222222222\n",
      "The training loss at 104th epoch : 225.02262192175573  Training Accuracy:0.5022222222222222\n",
      "The training loss at 105th epoch : 225.02255692169507  Training Accuracy:0.5022222222222222\n",
      "The training loss at 106th epoch : 225.02249221612863  Training Accuracy:0.5022222222222222\n",
      "The training loss at 107th epoch : 225.022427803216  Training Accuracy:0.5022222222222222\n",
      "The training loss at 108th epoch : 225.02236368113165  Training Accuracy:0.5022222222222222\n",
      "The training loss at 109th epoch : 225.0222998480646  Training Accuracy:0.5022222222222222\n",
      "The training loss at 110th epoch : 225.0222363022183  Training Accuracy:0.5011111111111111\n",
      "The training loss at 111th epoch : 225.02217304181056  Training Accuracy:0.5011111111111111\n",
      "The training loss at 112th epoch : 225.02211006507324  Training Accuracy:0.5011111111111111\n",
      "The training loss at 113th epoch : 225.02204737025238  Training Accuracy:0.5011111111111111\n",
      "The training loss at 114th epoch : 225.02198495560782  Training Accuracy:0.5011111111111111\n",
      "The training loss at 115th epoch : 225.02192281941325  Training Accuracy:0.5011111111111111\n",
      "The training loss at 116th epoch : 225.0218609599559  Training Accuracy:0.5011111111111111\n",
      "The training loss at 117th epoch : 225.02179937553646  Training Accuracy:0.5011111111111111\n",
      "The training loss at 118th epoch : 225.0217380644691  Training Accuracy:0.5011111111111111\n",
      "The training loss at 119th epoch : 225.0216770250812  Training Accuracy:0.5\n",
      "The training loss at 120th epoch : 225.02161625571324  Training Accuracy:0.5\n",
      "The training loss at 121th epoch : 225.02155575471866  Training Accuracy:0.5\n",
      "The training loss at 122th epoch : 225.02149552046376  Training Accuracy:0.5\n",
      "The training loss at 123th epoch : 225.02143555132764  Training Accuracy:0.5\n",
      "The training loss at 124th epoch : 225.02137584570198  Training Accuracy:0.5\n",
      "The training loss at 125th epoch : 225.02131640199093  Training Accuracy:0.5\n",
      "The training loss at 126th epoch : 225.0212572186111  Training Accuracy:0.5\n",
      "The training loss at 127th epoch : 225.0211982939913  Training Accuracy:0.5011111111111111\n",
      "The training loss at 128th epoch : 225.02113962657245  Training Accuracy:0.5011111111111111\n",
      "The training loss at 129th epoch : 225.0210812148076  Training Accuracy:0.5011111111111111\n",
      "The training loss at 130th epoch : 225.02102305716167  Training Accuracy:0.5011111111111111\n",
      "The training loss at 131th epoch : 225.0209651521114  Training Accuracy:0.5011111111111111\n",
      "The training loss at 132th epoch : 225.02090749814522  Training Accuracy:0.5011111111111111\n",
      "The training loss at 133th epoch : 225.0208500937631  Training Accuracy:0.5011111111111111\n",
      "The training loss at 134th epoch : 225.02079293747653  Training Accuracy:0.5011111111111111\n",
      "The training loss at 135th epoch : 225.02073602780848  Training Accuracy:0.5011111111111111\n",
      "The training loss at 136th epoch : 225.02067936329294  Training Accuracy:0.5011111111111111\n",
      "The training loss at 137th epoch : 225.02062294247528  Training Accuracy:0.5011111111111111\n",
      "The training loss at 138th epoch : 225.02056676391186  Training Accuracy:0.5011111111111111\n",
      "The training loss at 139th epoch : 225.02051082616987  Training Accuracy:0.5011111111111111\n",
      "The training loss at 140th epoch : 225.02045512782755  Training Accuracy:0.5011111111111111\n",
      "The training loss at 141th epoch : 225.0203996674738  Training Accuracy:0.5011111111111111\n",
      "The training loss at 142th epoch : 225.02034444370815  Training Accuracy:0.5011111111111111\n",
      "The training loss at 143th epoch : 225.02028945514076  Training Accuracy:0.5022222222222222\n",
      "The training loss at 144th epoch : 225.02023470039214  Training Accuracy:0.5011111111111111\n",
      "The training loss at 145th epoch : 225.02018017809323  Training Accuracy:0.5022222222222222\n",
      "The training loss at 146th epoch : 225.02012588688524  Training Accuracy:0.5022222222222222\n",
      "The training loss at 147th epoch : 225.0200718254195  Training Accuracy:0.5022222222222222\n",
      "The training loss at 148th epoch : 225.02001799235757  Training Accuracy:0.5022222222222222\n",
      "The training loss at 149th epoch : 225.01996438637076  Training Accuracy:0.5022222222222222\n",
      "The training loss at 150th epoch : 225.0199110061405  Training Accuracy:0.5022222222222222\n",
      "The training loss at 151th epoch : 225.01985785035782  Training Accuracy:0.5022222222222222\n",
      "The training loss at 152th epoch : 225.01980491772372  Training Accuracy:0.5033333333333333\n",
      "The training loss at 153th epoch : 225.01975220694857  Training Accuracy:0.5033333333333333\n",
      "The training loss at 154th epoch : 225.01969971675248  Training Accuracy:0.5033333333333333\n",
      "The training loss at 155th epoch : 225.0196474458649  Training Accuracy:0.5033333333333333\n",
      "The training loss at 156th epoch : 225.0195953930247  Training Accuracy:0.5022222222222222\n",
      "The training loss at 157th epoch : 225.01954355698  Training Accuracy:0.5022222222222222\n",
      "The training loss at 158th epoch : 225.01949193648818  Training Accuracy:0.5022222222222222\n",
      "The training loss at 159th epoch : 225.01944053031573  Training Accuracy:0.5022222222222222\n",
      "The training loss at 160th epoch : 225.0193893372381  Training Accuracy:0.5022222222222222\n",
      "The training loss at 161th epoch : 225.0193383560398  Training Accuracy:0.5022222222222222\n",
      "The training loss at 162th epoch : 225.01928758551418  Training Accuracy:0.5022222222222222\n",
      "The training loss at 163th epoch : 225.01923702446334  Training Accuracy:0.5022222222222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 164th epoch : 225.0191866716982  Training Accuracy:0.5022222222222222\n",
      "The training loss at 165th epoch : 225.01913652603827  Training Accuracy:0.5022222222222222\n",
      "The training loss at 166th epoch : 225.01908658631157  Training Accuracy:0.5022222222222222\n",
      "The training loss at 167th epoch : 225.01903685135474  Training Accuracy:0.5022222222222222\n",
      "The training loss at 168th epoch : 225.01898732001268  Training Accuracy:0.5022222222222222\n",
      "The training loss at 169th epoch : 225.0189379911388  Training Accuracy:0.5022222222222222\n",
      "The training loss at 170th epoch : 225.01888886359467  Training Accuracy:0.5011111111111111\n",
      "The training loss at 171th epoch : 225.01883993625003  Training Accuracy:0.5011111111111111\n",
      "The training loss at 172th epoch : 225.01879120798284  Training Accuracy:0.5011111111111111\n",
      "The training loss at 173th epoch : 225.01874267767906  Training Accuracy:0.5011111111111111\n",
      "The training loss at 174th epoch : 225.01869434423259  Training Accuracy:0.5011111111111111\n",
      "The training loss at 175th epoch : 225.01864620654536  Training Accuracy:0.5011111111111111\n",
      "The training loss at 176th epoch : 225.018598263527  Training Accuracy:0.5011111111111111\n",
      "The training loss at 177th epoch : 225.01855051409504  Training Accuracy:0.5011111111111111\n",
      "The training loss at 178th epoch : 225.01850295717463  Training Accuracy:0.5011111111111111\n",
      "The training loss at 179th epoch : 225.01845559169857  Training Accuracy:0.5011111111111111\n",
      "The training loss at 180th epoch : 225.0184084166073  Training Accuracy:0.5011111111111111\n",
      "The training loss at 181th epoch : 225.0183614308487  Training Accuracy:0.5011111111111111\n",
      "The training loss at 182th epoch : 225.01831463337808  Training Accuracy:0.5011111111111111\n",
      "The training loss at 183th epoch : 225.0182680231582  Training Accuracy:0.5011111111111111\n",
      "The training loss at 184th epoch : 225.01822159915915  Training Accuracy:0.5\n",
      "The training loss at 185th epoch : 225.0181753603581  Training Accuracy:0.5\n",
      "The training loss at 186th epoch : 225.01812930573962  Training Accuracy:0.5\n",
      "The training loss at 187th epoch : 225.0180834342953  Training Accuracy:0.4988888888888889\n",
      "The training loss at 188th epoch : 225.01803774502378  Training Accuracy:0.4988888888888889\n",
      "The training loss at 189th epoch : 225.01799223693078  Training Accuracy:0.4988888888888889\n",
      "The training loss at 190th epoch : 225.01794690902892  Training Accuracy:0.4988888888888889\n",
      "The training loss at 191th epoch : 225.01790176033768  Training Accuracy:0.4988888888888889\n",
      "The training loss at 192th epoch : 225.01785678988347  Training Accuracy:0.4988888888888889\n",
      "The training loss at 193th epoch : 225.01781199669938  Training Accuracy:0.4988888888888889\n",
      "The training loss at 194th epoch : 225.0177673798252  Training Accuracy:0.4988888888888889\n",
      "The training loss at 195th epoch : 225.01772293830743  Training Accuracy:0.4988888888888889\n",
      "The training loss at 196th epoch : 225.0176786711992  Training Accuracy:0.4988888888888889\n",
      "The training loss at 197th epoch : 225.0176345775601  Training Accuracy:0.4988888888888889\n",
      "The training loss at 198th epoch : 225.01759065645626  Training Accuracy:0.4988888888888889\n",
      "The training loss at 199th epoch : 225.01754690696026  Training Accuracy:0.4988888888888889\n"
     ]
    }
   ],
   "source": [
    "train_gradient_update(model , loss_m , X_train,Y_train , 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_neural_net(model,X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at 0th epoch : 225.017503328151  Training Accuracy:0.4988888888888889\n",
      "The training loss at 1th epoch : 450.0  Training Accuracy:0.5\n"
     ]
    }
   ],
   "source": [
    "train_newton_update(model , loss_m , X_train,Y_train , 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_neural_net(model,X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
